{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"id":"LZpSOJA2wgeL","executionInfo":{"status":"ok","timestamp":1671426356598,"user_tz":-180,"elapsed":462,"user":{"displayName":"Riantsoa Rasahivelo","userId":"10121465007685899405"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","import pandas as pd\n","from collections import Counter\n","import argparse\n","import numpy as np\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"KlmdUHpYwgeO","executionInfo":{"status":"ok","timestamp":1671426359178,"user_tz":-180,"elapsed":492,"user":{"displayName":"Riantsoa Rasahivelo","userId":"10121465007685899405"}}},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, dataset):\n","        super(Model, self).__init__()\n","        self.lstm_size = 128\n","        self.embedding_dim = 128\n","        self.num_layers = 3\n","\n","        n_vocab = len(dataset.uniq_words)\n","        self.embedding = nn.Embedding(\n","            num_embeddings=n_vocab,\n","            embedding_dim=self.embedding_dim,\n","        )\n","        self.lstm = nn.LSTM(\n","            input_size=self.lstm_size,\n","            hidden_size=self.lstm_size,\n","            num_layers=self.num_layers,\n","            dropout=0.2,\n","        )\n","        self.fc = nn.Linear(self.lstm_size, n_vocab)\n","\n","    def forward(self, x, prev_state):\n","        embed = self.embedding(x)\n","        output, state = self.lstm(embed, prev_state)\n","        logits = self.fc(output)\n","        return logits, state\n","\n","    def init_state(self, sequence_length):\n","        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n","                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"fRxGmGAGwgeR","executionInfo":{"status":"ok","timestamp":1671426362420,"user_tz":-180,"elapsed":434,"user":{"displayName":"Riantsoa Rasahivelo","userId":"10121465007685899405"}}},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(\n","        self,\n","        args,\n","    ):\n","        self.args = args\n","        self.words = self.load_words()\n","        self.uniq_words = self.get_uniq_words()\n","\n","        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n","        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n","\n","        self.words_indexes = [self.word_to_index[w] for w in self.words]\n","\n","    #def load_words(self):\n","     #   train_df = pd.read_csv('data/reddit-cleanjokes.csv')\n","      #  text = train_df['Joke'].str.cat(sep=' ')\n","       # return text.split(' ')\n","    def load_words(self):\n","        with open('erickManana.txt', 'r',encoding='utf-8') as f:\n","            text = f.read()\n","        return text.split(' ')\n","\n","    def get_uniq_words(self):\n","        word_counts = Counter(self.words)\n","        return sorted(word_counts, key=word_counts.get, reverse=True)\n","\n","    def __len__(self):\n","        return len(self.words_indexes) - self.args.sequence_length\n","\n","    def __getitem__(self, index):\n","        return (\n","            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n","            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n","        )"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"b4iQFF3GwgeT","executionInfo":{"status":"ok","timestamp":1671426368908,"user_tz":-180,"elapsed":434,"user":{"displayName":"Riantsoa Rasahivelo","userId":"10121465007685899405"}}},"outputs":[],"source":["def train(dataset, model, args):\n","    model.train()\n","\n","    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(args.max_epochs):\n","        state_h, state_c = model.init_state(args.sequence_length)\n","\n","        for batch, (x, y) in enumerate(dataloader):\n","            optimizer.zero_grad()\n","\n","            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n","            loss = criterion(y_pred.transpose(1, 2), y)\n","\n","            state_h = state_h.detach()\n","            state_c = state_c.detach()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"-DSPkj1DwgeU","executionInfo":{"status":"ok","timestamp":1671426372110,"user_tz":-180,"elapsed":482,"user":{"displayName":"Riantsoa Rasahivelo","userId":"10121465007685899405"}}},"outputs":[],"source":["def predict(dataset, model, text, next_words=100):\n","    model.eval()\n","\n","    words = text.split(' ')\n","    state_h, state_c = model.init_state(len(words))\n","\n","    for i in range(0, next_words):\n","        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n","        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n","\n","        last_word_logits = y_pred[0][-1]\n","        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n","        word_index = np.random.choice(len(last_word_logits), p=p)\n","        words.append(dataset.index_to_word[word_index])\n","\n","    return words"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"id":"1ajzPYDLwgeW","executionInfo":{"status":"error","timestamp":1671426414779,"user_tz":-180,"elapsed":534,"user":{"displayName":"Riantsoa Rasahivelo","userId":"10121465007685899405"}},"outputId":"d418a52c-48b1-48fd-aeab-dc290bb5b679"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-0d5681cc075f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-fe4c629f75c7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      5\u001b[0m     ):\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniq_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uniq_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-fe4c629f75c7>\u001b[0m in \u001b[0;36mload_words\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m        \u001b[0;31m# return text.split(' ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'erickManana.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'erickManana.txt'"]}],"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--max-epochs', type=int, default=100)\n","parser.add_argument('--batch-size', type=int, default=64)\n","parser.add_argument('--sequence-length', type=int, default=4)\n","#args = parser.parse_args()\n","args, unknown = parser.parse_known_args()\n","\n","dataset = Dataset(args)\n","model = Model(dataset)\n","\n","train(dataset, model, args)\n","print(predict(dataset, model, text='misy fitia'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLtsd8XdwgeX","outputId":"f886b634-fd62-4efa-d077-645a72e09402"},"outputs":[{"name":"stdout","output_type":"stream","text":["['tsy', 'mety', 'maty', 'é', 'taloha', 'revirevinay', 'fidoboka', 'sendra', 'havo', 'soa', 'eroa', 'ny', 'nampianatra', 'miovaova', 'ny', 'zanako', 'no', 'raha', 'maty', 'lava', 'ny', 'cvndrandria', 'ka', 'iza', 'ny', 'azo', 'mitempo', 'dia', 'raiso', 'ka', 'ilofosako', 'tomany', 'sy', 'bebe', 'izaho', 'raha', 'mbola', 'irery', 'ka', 'ny', 'hanitra', 'feno', 'iainana', 'fa', 'tena', 'mangirifiry', 'any', 'tsotra', 'sady', 'hitako', 'ianao', 'ampo', 'izaho', 'matetika', 'hankina', 'zavatr’olona', 'na', 'ady', 'mitempo', 'efa', 'lalina', 'hatreto', 'dia', 'mba', 'hamelaho', 'na', 'nivatofantsika', 'sarobidy', 'mena', 'tafo', 'orohana', 'a', 'ny', 'hiravahanny', 'mena', 'ny', 'misangy', 'na', 'roa', 'maty', 'ny', 'soa', 'sasatra', 'fa', 'ny', 'voizo', 'x', 'ny', 'rehefa', 'mitempo', 'mba', 'reko', 'ny', 'ampahazahoy', 'ilay', 'fonao', 'ee', 'handeha', 'gasikara', 'mankarary', 'matetika', 'fa', 'fiainana']\n"]}],"source":["print(predict(dataset, model, text='tsy mety maty'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVmOUa2FwgeY"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}